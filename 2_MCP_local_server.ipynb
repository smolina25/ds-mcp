{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db19059a",
   "metadata": {},
   "source": [
    "# MCP using local servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbc06b",
   "metadata": {},
   "source": [
    "In this notebook, we will see\n",
    "- Chain of Thought prompting for tools\n",
    "- Running a local http mcp server\n",
    "  - Connecting to http server as a client\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a6113",
   "metadata": {},
   "source": [
    "Inside the `mcp_servers` folder, we have another mcp tools file `mcp_http_server.py`.\n",
    "\n",
    "In this python file or mcp server file, we have two mcp tools\n",
    "1. the `read_local_file` mcp tool - which is same as the one used in notebook [1_intro_to_MCP](1_intro_to_MCP.ipynb)\n",
    "2. A new mcp tool called `execute_python_code`\n",
    "   1. We can use this tool to directly run python code\n",
    "\n",
    "We will combine both tools together.\n",
    "1. We will read a python script from local machine via the mcp tool `read_local_file`\n",
    "2. Then we will send the output of the first tool to the second tool which is `execute_python_code`\n",
    "3. We will us Chain of Thought prompting for this execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa2764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client as http_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfdd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_SERVER_URL = \"http://127.0.0.1:8000/sse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802420be",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "                \"You are a helpful assistant with access to tools. \"\n",
    "                \"If a task requires multiple steps (e.g., reading a file then running it), \"\n",
    "                \"perform them one by one. Use the output of one tool to inform the next. \"\n",
    "                \"If you need to read a file, use the file reading tool first. If you need to execute code, use the code execution tool.\"\n",
    "                \"After getting result from executing code, give the output as the final answer.\"\n",
    "                \"When you have the final answer, summarize it for the user.\"\n",
    "                \"If the python files have syntax error then output the syntax error as the final answer without trying to execute it and tell what the error is\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a00109",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chainable_agent(user_prompt, system_prompt, model):\n",
    "    # Define the conversation history\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "    ]\n",
    "\n",
    "    async with http_client(LOCAL_SERVER_URL) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # 1. Fetch and format tools for Ollama\n",
    "            tools_result = await session.list_tools()\n",
    "            ollama_tools = [{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description,\n",
    "                    \"parameters\": t.inputSchema,\n",
    "                }\n",
    "            } for t in tools_result.tools]\n",
    "\n",
    "            print(f\"üöÄ Starting agent loop for: '{user_prompt}'\")\n",
    "            \n",
    "            # 2. Start the Loop (Max 5 turns to prevent infinite loops)\n",
    "            for turn in range(5):\n",
    "                print(f\"\\n--- Turn {turn + 1} ---\")\n",
    "                \n",
    "                response = ollama.chat(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    tools=ollama_tools\n",
    "                )\n",
    "                \n",
    "                assistant_msg = response['message']\n",
    "                messages.append(assistant_msg) # Record the model's reasoning/call\n",
    "\n",
    "                # Check if the model wants to call tools\n",
    "                if not assistant_msg.get('tool_calls'):\n",
    "                    print(\"‚úÖ Final answer reached.\")\n",
    "                    return assistant_msg['content']\n",
    "\n",
    "                # 3. Execute Tool Calls\n",
    "                for tool_call in assistant_msg['tool_calls']:\n",
    "                    t_name = tool_call['function']['name']\n",
    "                    t_args = tool_call['function']['arguments']\n",
    "                    \n",
    "                    print(f\"üõ†Ô∏è Tool Call: {t_name}({t_args})\")\n",
    "                    \n",
    "                    # Execute the tool via MCP\n",
    "                    result = await session.call_tool(t_name, t_args)\n",
    "                    \n",
    "                    # Append the result back to the conversation\n",
    "                    messages.append({\n",
    "                        'role': 'tool',\n",
    "                        'content': str(result.content),\n",
    "                        'name': t_name\n",
    "                    })\n",
    "                    print(f\"üì• Tool Result received.\")\n",
    "\n",
    "            return \"‚ö†Ô∏è Loop limit reached without a final answer.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a42cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting agent loop for: 'Read fibonacci_numbers.py in the scripts folder and then execute it. Path to file=scripts/fibonacci_numbers.py'\n",
      "\n",
      "--- Turn 1 ---\n",
      "üõ†Ô∏è Tool Call: execute_python_code({'file_path': {'type': 'string', 'value': 'scripts/fibonacci_numbers.py'}})\n",
      "üì• Tool Result received.\n",
      "\n",
      "--- Turn 2 ---\n",
      "‚úÖ Final answer reached.\n",
      "\n",
      "RESULT:\n",
      "The file `fibonacci_numbers.py` in the `scripts` folder has a syntax error. The error is that the `file_path` argument for the `execute_python_code` tool should be a valid string, but the provided path is missing a forward slash between the directory name and the file name.\n",
      "\n",
      "To fix this, you can modify the file path to include the correct separation:\n",
      "\n",
      "```\n",
      "file_path=\"scripts/fibonacci_numbers.py\"\n",
      "```\n",
      "\n",
      "Without the forward slash, Python treats `s` as an extension, not a directory separator. With the forward slash, Python correctly identifies the directory and file paths.\n"
     ]
    }
   ],
   "source": [
    "query = \"Read fibonacci_numbers.py in the scripts folder and then execute it. Path to file=scripts/fibonacci_numbers.py\"\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "result = await run_chainable_agent(query, SYSTEM_PROMPT, MODEL)\n",
    "print(f\"\\nRESULT:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa37903",
   "metadata": {},
   "source": [
    "When we run the cell above we get an error because our mcp tool is not active on the server. For this we first have to start the local server. \n",
    "To do this:\n",
    "1. Open terminal\n",
    "2. Navigate to this repository\n",
    "3. Activate your virtual environment\n",
    "4. Run `python mcp_servers/mcp_http_server.py`\n",
    "\n",
    "\n",
    "After executing this command, you will see that the server has started and now actively listening for connections\n",
    "\n",
    "#### Check if the server is listening and we can connect to the server\n",
    "1. Open another terminal and navigate to the repository\n",
    "2. Activate the virtual environment\n",
    "3. Run `curl -v http://127.0.0.1:8000/sse`\n",
    "4. If the connection is established to the server, you should see something like `HTTP/1.1 200 OK` on your terminal\n",
    "5. On the server side, that is the terminal window where server is running, you will also see `200 OK` if successfully connected\n",
    "6. Exit the curl command on the client side with `control + c/d`\n",
    "\n",
    "\n",
    "#### Connect LLM to the MCP server\n",
    "Re-ru the above cell again, to see if we can connect to the server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57400b",
   "metadata": {},
   "source": [
    "## DIY - Do It Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7790f",
   "metadata": {},
   "source": [
    "Now try to implement new mcp tools and make them work for you. You can start with very simple math tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2865671e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools: ['add_numbers', 'subtract_numbers', 'multiply_numbers', 'divide_numbers', 'factorial']\n",
      "add_numbers: [TextContent(type='text', text='20.0', annotations=None, meta=None)]\n",
      "factorial: [TextContent(type='text', text='720', annotations=None, meta=None)]\n",
      "divide_numbers: [TextContent(type='text', text='Error: division by zero is not allowed.', annotations=None, meta=None)]\n"
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client as http_client\n",
    "\n",
    "LOCAL_MATH_SERVER_URL = \"http://127.0.0.1:8001/sse\"\n",
    "\n",
    "async def test_math_tools():\n",
    "    async with http_client(LOCAL_MATH_SERVER_URL) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            tools_result = await session.list_tools()\n",
    "            print(\"Available tools:\", [t.name for t in tools_result.tools])\n",
    "\n",
    "            r1 = await session.call_tool(\"add_numbers\", {\"a\": 12, \"b\": 8})\n",
    "            r2 = await session.call_tool(\"factorial\", {\"n\": 6})\n",
    "            r3 = await session.call_tool(\"divide_numbers\", {\"a\": 10, \"b\": 0})\n",
    "\n",
    "            print(\"add_numbers:\", r1.content)\n",
    "            print(\"factorial:\", r2.content)\n",
    "            print(\"divide_numbers:\", r3.content)\n",
    "\n",
    "await test_math_tools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12184824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the computations are:\n",
      "\n",
      "(15 * 4) - 9 = 60\n",
      "Factorial of 5 = 120\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client as http_client\n",
    "\n",
    "LOCAL_MATH_SERVER_URL = \"http://127.0.0.1:8001/sse\"\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a math assistant with tool access. \"\n",
    "    \"Use tools for calculations. \"\n",
    "    \"If multiple steps are needed, do them one by one. \"\n",
    "    \"Return a clean final answer.\"\n",
    ")\n",
    "\n",
    "async def run_math_agent(user_prompt, model=MODEL):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    async with http_client(LOCAL_MATH_SERVER_URL) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            tools_result = await session.list_tools()\n",
    "            ollama_tools = [{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description,\n",
    "                    \"parameters\": t.inputSchema,\n",
    "                }\n",
    "            } for t in tools_result.tools]\n",
    "\n",
    "            for _ in range(6):\n",
    "                response = ollama.chat(model=model, messages=messages, tools=ollama_tools)\n",
    "                assistant_msg = response[\"message\"]\n",
    "                messages.append(assistant_msg)\n",
    "\n",
    "                if not assistant_msg.get(\"tool_calls\"):\n",
    "                    return assistant_msg[\"content\"]\n",
    "\n",
    "                for tool_call in assistant_msg[\"tool_calls\"]:\n",
    "                    t_name = tool_call[\"function\"][\"name\"]\n",
    "                    t_args = tool_call[\"function\"][\"arguments\"]\n",
    "                    if isinstance(t_args, str):\n",
    "                        t_args = json.loads(t_args)\n",
    "\n",
    "                    result = await session.call_tool(t_name, t_args)\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": t_name,\n",
    "                        \"content\": str(result.content),\n",
    "                    })\n",
    "\n",
    "    return \"No final answer produced.\"\n",
    "\n",
    "query = \"Compute (15 * 4) - 9, then compute factorial of 5, and show both results.\"\n",
    "result = await run_math_agent(query)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
