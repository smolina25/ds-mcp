{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9590c796",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bfd7e",
   "metadata": {},
   "source": [
    "We can use any LLM with MCP. In this repository we will use `llama3.2:1b` and `llama3.2:3b`\n",
    "\n",
    "You can also download other models such as `llama3.2:8b` (if your machine supports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f05399",
   "metadata": {},
   "source": [
    "| Model | Command |\n",
    "| --- | --- |\n",
    "| llama 3.2:1b | `ollama pull llama3.2:1b` |\n",
    "| llama 3.2:3b | `ollama pull llama3.2:3b` |\n",
    "| llama 3.2:8b | `ollama pull llama3.2:8b` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea24ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b494d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_MODEL='llama3.2:3b' # or llama3.2:1b for faster but less accurate results\n",
    "SMALL_MODEL='llama3.2:1b' # or llama3.2:3b for better but slower results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a1c56",
   "metadata": {},
   "source": [
    "We have a folder `mcp_servers`, inside this folder we have our mcp tool written under different python scripts. These scripts are also called servers in some cases (when these are running on a remote server waiting for a client connection). Because MCP tools can be used as plug and play unlike agents, these scripts can run on remote servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44171f8",
   "metadata": {},
   "source": [
    "First, we have a very simple mcp tool which is a tool to add two numbers under the script `the_math_server.py`.\n",
    "We will use this tool first just to understand how the mcp works with an LLM (in this case llama3.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74520f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for an MCP Server (Example: a local python script)\n",
    "# If you don't have one, you can use a simple 'hello-world' MCP server script\n",
    "\n",
    "MCP_SERVER_PATH = os.path.abspath(\"./mcp_servers/the_math_server.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48a9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_mcp_query(mcp_server_path, user_prompt, model):\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[mcp_server_path],\n",
    "    )\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # 1. Initialize session\n",
    "            await session.initialize()\n",
    "            \n",
    "            # 2. List available tools from the MCP server\n",
    "            tools = await session.list_tools()\n",
    "            \n",
    "            # 3. Ask Ollama to decide which tool to use\n",
    "            # Note: We format the MCP tools into Ollama's tool format\n",
    "            ollama_tools = [\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": t.name,\n",
    "                        \"description\": t.description,\n",
    "                        \"parameters\": t.inputSchema,\n",
    "                    },\n",
    "                }\n",
    "                for t in tools.tools\n",
    "            ]\n",
    "\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{'role': 'user', 'content': user_prompt}],\n",
    "                tools=ollama_tools,\n",
    "            )\n",
    "\n",
    "            # 4. Handle Tool Calls\n",
    "            if response.get('message', {}).get('tool_calls'):\n",
    "                for tool_call in response['message']['tool_calls']:\n",
    "                    tool_name = tool_call['function']['name']\n",
    "                    tool_args = tool_call['function']['arguments']\n",
    "                    \n",
    "                    print(f\"üõ†Ô∏è Calling tool: {tool_name} with {tool_args}\")\n",
    "                    \n",
    "                    # Execute tool via MCP\n",
    "                    result = await session.call_tool(tool_name, tool_args)\n",
    "                    print(f\"‚úÖ Tool Result: {result.content}\")\n",
    "                    \n",
    "                    # Final response from LLM\n",
    "                    final_response = ollama.chat(\n",
    "                        model=model,\n",
    "                        messages=[\n",
    "                            {'role': 'user', 'content': user_prompt},\n",
    "                            response['message'],\n",
    "                            {'role': 'tool', 'content': str(result.content), 'name': tool_name}\n",
    "                        ]\n",
    "                    )\n",
    "                    return final_response['message']['content']\n",
    "            \n",
    "            return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609f49a",
   "metadata": {},
   "source": [
    "### Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc616abc",
   "metadata": {},
   "source": [
    "`async def run_mcp_query(user_prompt, MODEL):` \n",
    "- The word async (asynchronous) just means your computer won't freeze up while it waits for the process to finish. It can go do other chores, like a person putting a phone on speaker while waiting on hold.\n",
    "- The Layman Translation: \"I am preparing to ask my assistant a question (the user_prompt), and I'm willing to wait patiently for the answer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fb515",
   "metadata": {},
   "source": [
    "`async with stdio_client(server_params) as (read, write):`\n",
    "- This step builds the actual physical/digital pipe between your main program and the hidden tool server. The server_params are like the phone number or the address of the office.\n",
    "- The Layman Translation: \"I am plugging a secure phone line directly into the assistant's office. I have an earpiece to listen (read) and a microphone to speak (write).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a16179",
   "metadata": {},
   "source": [
    "`async with ClientSession(read, write) as session:`\n",
    "- Just having a wire isn't enough; the computers need rules on how to format their messages. A ClientSession wraps the raw wire in a polite protocol (like agreeing to say \"Over\" when you finish speaking on a walkie-talkie).\n",
    "- The Layman Translation: \"Now that the wire is connected, we are agreeing to speak the same language so we don't talk over each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d3eee",
   "metadata": {},
   "source": [
    "`await session.initialize()`\n",
    "- Before you can ask your actual question or ask the server to run tools, you have to do a digital handshake. This proves the server is awake, functioning, and ready to accept commands.\n",
    "- The Layman Translation: \"I am saying 'Hello, are you there and ready to work?' and I will wait (await) until you say 'Yes, I am ready!'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9df04",
   "metadata": {},
   "source": [
    "Let us now try to use our MCP tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c121c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 5 + 5?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01836b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Calling tool: add_numbers with {'a': '5', 'b': '5'}\n",
      "‚úÖ Tool Result: [TextContent(type='text', text='10', annotations=None, meta=None)]\n",
      "LLM Final Response: I cannot provide a definitive answer to every question. If you want to know the current date and time, I can help with that.\n"
     ]
    }
   ],
   "source": [
    "result = await run_mcp_query(MCP_SERVER_PATH,query, SMALL_MODEL)\n",
    "print(f\"LLM Final Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfaa18f",
   "metadata": {},
   "source": [
    "You will see above that the `SMALL_MODEL` fails to give the answer to a very simple question. Though the process successfully uses the `Math` MCP tool and the result was also calculated correctly as can be seen in the 2nd line of the output under `text='10'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5cf46",
   "metadata": {},
   "source": [
    "Lets now try the same query with the `LARGE_MODEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24b7df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Calling tool: add_numbers with {'a': 5, 'b': 5}\n",
      "‚úÖ Tool Result: [TextContent(type='text', text='10', annotations=None, meta=None)]\n",
      "LLM Final Response: The answer to 5 + 5 is 10.\n"
     ]
    }
   ],
   "source": [
    "result = await run_mcp_query(MCP_SERVER_PATH, query, LARGE_MODEL)\n",
    "print(f\"LLM Final Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21f4bd",
   "metadata": {},
   "source": [
    "## Run a more complex MCP servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e01dc",
   "metadata": {},
   "source": [
    "There is another server file `mcp_tools_server.py` inside the `mcp_servers` folder. \n",
    "\n",
    "This mcp server scripts consists of two tools.\n",
    "1. Web Search Tool - using DuckDuckGo \n",
    "2. Reading a local file Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99971d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_SERVER_PATH = os.path.abspath(\"./mcp_servers/mcp_tools_server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e4b45",
   "metadata": {},
   "source": [
    "Lets first try to use the `web_search` tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59aa370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Argentina will play Uruguay for the final in the 2026 FIFA World Cup.\"\n"
     ]
    }
   ],
   "source": [
    "# Try a web search:\n",
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "print(await run_mcp_query(MCP_SERVER_PATH,query, SMALL_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fc722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Calling tool: search_web with {'query': 'FIFA World Cup winner 2026'}\n",
      "‚úÖ Tool Result: [TextContent(type='text', text='No results found.', annotations=None, meta=None)]\n",
      "The 2026 FIFA World Cup has not yet taken place. The next FIFA World Cup is scheduled to take place in 2026 in the United States, Canada, and Mexico.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "print(await run_mcp_query(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831466a",
   "metadata": {},
   "source": [
    "Now lets use the `read_local_file` tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831f4759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Calling tool: read_local_file with {'file_path': 'ai_response.txt'}\n",
      "‚úÖ Tool Result: [TextContent(type='text', text=\"Oh boy, let me explain something cool to you!\\n\\nImagine you have a big Lego castle, and you want to build it really, really tall. But instead of using your own hands, you need someone else to help you make sure everything is just right.\\n\\nThat's kind of like what an AI Project Manager does! They're like the grown-up in charge of building super cool projects, like video games or websites.\\n\\nTheir job is to:\\n\\n* Plan out all the steps it takes to build a project\\n* Make sure everyone involved (like your friends and family) knows what they need to do\\n* Fix any mistakes that might happen along the way\\n* Help make sure everything gets built exactly right\\n\\nJust like how you have helpers in your Lego castle, an AI Project Manager has tools and people who can help them build their projects. And just as you might get feedback from others, an AI Project Manager might ask questions or show them things to make sure they're doing it just right.\\n\\nThe best part is, AI Project Managers can work with computers and apps that do all the hard work for them! It's like having a super-smart assistant who helps build your Lego castle without even breaking a sweat.\", annotations=None, meta=None)]\n",
      "('Here is a summary of the text:\\n'\n",
      " '\\n'\n",
      " 'The text describes an AI Project Manager as someone who plans and builds '\n",
      " 'projects, similar to building a Lego castle. They ensure everyone involved '\n",
      " 'knows their role, fix mistakes, and deliver high-quality results. The '\n",
      " 'project manager uses tools and people to help with the build and may ask for '\n",
      " 'feedback or show examples to get it just right.')\n"
     ]
    }
   ],
   "source": [
    "# Try reading a file\n",
    "query = \"Read the text inside the document called ai_response.txt and then make a summary of it\"\n",
    "pprint(await run_mcp_query(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c160a",
   "metadata": {},
   "source": [
    "Try to run the above code block again if facing errors or weird outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3fd06",
   "metadata": {},
   "source": [
    "The `SMALL_MODEL`, that is `llama3.2:1b` does not perform very well to even not so difficult tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3057be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Calling tool: read_local_file with {'file_path': 'ai_response.txt'}\n",
      "‚úÖ Tool Result: [TextContent(type='text', text=\"Oh boy, let me explain something cool to you!\\n\\nImagine you have a big Lego castle, and you want to build it really, really tall. But instead of using your own hands, you need someone else to help you make sure everything is just right.\\n\\nThat's kind of like what an AI Project Manager does! They're like the grown-up in charge of building super cool projects, like video games or websites.\\n\\nTheir job is to:\\n\\n* Plan out all the steps it takes to build a project\\n* Make sure everyone involved (like your friends and family) knows what they need to do\\n* Fix any mistakes that might happen along the way\\n* Help make sure everything gets built exactly right\\n\\nJust like how you have helpers in your Lego castle, an AI Project Manager has tools and people who can help them build their projects. And just as you might get feedback from others, an AI Project Manager might ask questions or show them things to make sure they're doing it just right.\\n\\nThe best part is, AI Project Managers can work with computers and apps that do all the hard work for them! It's like having a super-smart assistant who helps build your Lego castle without even breaking a sweat.\", annotations=None, meta=None)]\n",
      "('The contents of ai_response.txt seem to be a text response from an AI model. '\n",
      " 'The response is about AI Project Managers and how they help build projects '\n",
      " 'by planning out steps, ensuring everyone knows their role, fixing mistakes, '\n",
      " 'and ensuring everything gets built right.\\n'\n",
      " '\\n'\n",
      " \"Here's a summary:\\n\"\n",
      " '\\n'\n",
      " '- An AI Project Manager plans and executes projects.\\n'\n",
      " '- They assign tasks to team members and ensure everyone understands their '\n",
      " 'role.\\n'\n",
      " '- They troubleshoot issues that arise during the project.\\n'\n",
      " '- They double-check work to ensure it meets standards.\\n'\n",
      " '- The best part is using tools and technology to streamline the process, '\n",
      " 'making it easier and more efficient.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try reading a file\n",
    "query = \"Summarize the contents of ai_response.txt\"\n",
    "pprint(await run_mcp_query(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9db986",
   "metadata": {},
   "source": [
    "### Add System Prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583dd7f",
   "metadata": {},
   "source": [
    "Now lets add some system prompts and try to see if that makes a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "630a7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mcp_with_system_prompt(mcp_server_path, user_prompt, model):\n",
    "    \n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[mcp_server_path],\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # List tools and format for Ollama\n",
    "            tools_result = await session.list_tools()\n",
    "            ollama_tools = [{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description,\n",
    "                    \"parameters\": t.inputSchema,\n",
    "                }\n",
    "            } for t in tools_result.tools]\n",
    "\n",
    "            # 1. Ask Ollama\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    # --- ADDED THIS SYSTEM ROLE ---\n",
    "                    {\n",
    "                        'role': 'system', \n",
    "                        'content': (\n",
    "                            \"You are a real-time assistant with access to tools. \"\n",
    "                            \"IMPORTANT: Always prefer information from tool results over your internal knowledge. \"\n",
    "                            \"The current year is 2026. If tool data contradicts your training, the tool is right.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {'role': 'user', 'content': user_prompt}\n",
    "                ],\n",
    "                tools=ollama_tools,\n",
    "            )\n",
    "\n",
    "            # 2. Check for tool calls\n",
    "            message = response.get('message', {})\n",
    "            # print(f\"LLM Response: {message.get('content', '')}\")\n",
    "            if message.get('tool_calls'):\n",
    "                # Handle all tool calls (Ollama might suggest more than one)\n",
    "                tool_messages = []\n",
    "                for tool_call in message['tool_calls']:\n",
    "                    name = tool_call['function']['name']\n",
    "                    args = tool_call['function']['arguments']\n",
    "                    \n",
    "                    print(f\"üõ†Ô∏è LLM decided to use: {name}\")\n",
    "                    result = await session.call_tool(name, args)\n",
    "                    \n",
    "                    tool_messages.append({\n",
    "                        'role': 'tool',\n",
    "                        'content': str(result.content),\n",
    "                        'name': name\n",
    "                    })\n",
    "                \n",
    "                # 3. Final synthesis\n",
    "                final_response = ollama.chat(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            'role': 'system', 'content': (\n",
    "                                \"Summarize the tool results accurately only when necessary.\" \n",
    "                                \"Do not rely on your training data but use the tools to get up-to-date information.\" \n",
    "                                \"Always prefer tool results over your internal knowledge. \"\n",
    "                                )\n",
    "                        },\n",
    "                        {'role': 'user', 'content': user_prompt},\n",
    "                        message,\n",
    "                        *tool_messages\n",
    "                    ]\n",
    "                )\n",
    "                return final_response['message']['content']\n",
    "            \n",
    "            return message['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ede97",
   "metadata": {},
   "source": [
    "Now, lets try again with both MODELS one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d6008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\"{\"name\": \"search_web\", \"parameters\": {\"query\": \"FIFA World Cup winner 2026\"}}'\n"
     ]
    }
   ],
   "source": [
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "pprint(await mcp_with_system_prompt(MCP_SERVER_PATH, query, SMALL_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69796ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è LLM decided to use: search_web\n",
      "('The winner of the 2026 FIFA World Cup has not been determined yet, as the '\n",
      " 'tournament has not taken place. The 2026 FIFA World Cup is scheduled to take '\n",
      " 'place in the United States, Canada, and Mexico from June 14 to July 15, '\n",
      " '2026. The final result will be announced after the tournament has concluded.')\n"
     ]
    }
   ],
   "source": [
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "pprint(await mcp_with_system_prompt(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24afd985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è LLM decided to use: read_local_file\n",
      "('The file ./DS-MCP/ai_response.txt does not exist. The tool output indicates '\n",
      " 'that the file was not found. \\n'\n",
      " '\\n'\n",
      " 'If you want to read the contents of the file, you would need to ensure its '\n",
      " \"existence and then use a tool like `cat` or a programming language's file \"\n",
      " 'reading functionality to access its content.\\n'\n",
      " '\\n'\n",
      " 'Let me know if there is any other way I can assist you with this task.')\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize the contents of the text file ./DS-MCP/ai_response.txt\"\n",
    "pprint(await mcp_with_system_prompt(MCP_SERVER_PATH,query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a7c11",
   "metadata": {},
   "source": [
    "If you are getting json responses try changing system prompt. \n",
    "\n",
    "<details>\n",
    "    <summary>Try adding this (check only if needed)</summary>\n",
    "    \"You are an MCP Assistant. If you need information, use a tool. DO NOT write raw JSON in your response. If you call a tool, use the proper tool-call format.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb8ccc",
   "metadata": {},
   "source": [
    "### Getting simple python code from web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f76d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è LLM decided to use: search_web\n",
      "I apologize for the detour!\n",
      "\n",
      "Here is a simple Python function that calculates the first 10 numbers in the Fibonacci sequence:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return \"Input should be positive integer\"\n",
      "    elif n == 1:\n",
      "        return [0]\n",
      "    elif n == 2:\n",
      "        return [0, 1]\n",
      "    else:\n",
      "        fib_sequence = [0, 1]\n",
      "        while len(fib_sequence) < n:\n",
      "            fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
      "        return fib_sequence\n",
      "\n",
      "print(fibonacci(10))\n",
      "```\n",
      "\n",
      "This code defines a function called `fibonacci` that takes an integer `n` as input and returns the first `n` numbers in the Fibonacci sequence. The Fibonacci sequence is defined where each number is the sum of the two preceding ones, usually starting with 0 and 1.\n",
      "\n",
      "When you run this function with the argument `10`, it will print out the first 10 numbers in the Fibonacci sequence: `[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]`.\n"
     ]
    }
   ],
   "source": [
    "query = \"Search for python code for calculating the first 10 numbers of the Fibonacci sequence.\"\n",
    "print(await mcp_with_system_prompt(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d86dd",
   "metadata": {},
   "source": [
    "Copy paste the above code in a python file and name it as `fibonacci_numbers.py`. Create a folder `scripts` and move the python script under the `scripts` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bea26a",
   "metadata": {},
   "source": [
    "Try running this python script in the terminal to check if the script is working or not\n",
    "\n",
    "```bash\n",
    "python scripts/fibonacci_numbers.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
