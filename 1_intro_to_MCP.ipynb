{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9590c796",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bfd7e",
   "metadata": {},
   "source": [
    "We can use any LLM with MCP. In this repository we will use `llama3.2:1b` and `llama3.2:3b`\n",
    "\n",
    "You can also download other models such as `llama3.2:8b` (if your machine supports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f05399",
   "metadata": {},
   "source": [
    "| Model | Command |\n",
    "| --- | --- |\n",
    "| llama 3.2:1b | `ollama pull llama3.2:1b` |\n",
    "| llama 3.2:3b | `ollama pull llama3.2:3b` |\n",
    "| llama 3.2:8b | `ollama pull llama3.2:8b` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b494d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_MODEL='llama3.2:3b' # or llama3.2:1b for faster but less accurate results\n",
    "SMALL_MODEL='llama3.2:1b' # or llama3.2:3b for better but slower results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a1c56",
   "metadata": {},
   "source": [
    "We have a folder `mcp_servers`, inside this folder we have our mcp tool written under different python scripts. These scripts are also called servers in some cases (when these are running on a remote server waiting for a client connection). Because MCP tools can be used as plug and play unlike agents, these scripts can run on remote servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44171f8",
   "metadata": {},
   "source": [
    "First, we have a very simple mcp tool which is a tool to add two numbers under the script `the_math_server.py`.\n",
    "We will use this tool first just to understand how the mcp works with an LLM (in this case llama3.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74520f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for an MCP Server (Example: a local python script)\n",
    "# If you don't have one, you can use a simple 'hello-world' MCP server script\n",
    "\n",
    "MCP_SERVER_PATH = os.path.abspath(\"./mcp_servers/the_math_server.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_mcp_query(mcp_server_path, user_prompt, model):\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[mcp_server_path],\n",
    "    )\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # 1. Initialize session\n",
    "            await session.initialize()\n",
    "            \n",
    "            # 2. List available tools from the MCP server\n",
    "            tools = await session.list_tools()\n",
    "            \n",
    "            # 3. Ask Ollama to decide which tool to use\n",
    "            # Note: We format the MCP tools into Ollama's tool format\n",
    "            ollama_tools = [\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": t.name,\n",
    "                        \"description\": t.description,\n",
    "                        \"parameters\": t.inputSchema,\n",
    "                    },\n",
    "                }\n",
    "                for t in tools.tools\n",
    "            ]\n",
    "\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[{'role': 'user', 'content': user_prompt}],\n",
    "                tools=ollama_tools,\n",
    "            )\n",
    "\n",
    "            # 4. Handle Tool Calls\n",
    "            if response.get('message', {}).get('tool_calls'):\n",
    "                for tool_call in response['message']['tool_calls']:\n",
    "                    tool_name = tool_call['function']['name']\n",
    "                    tool_args = tool_call['function']['arguments']\n",
    "                    \n",
    "                    print(f\"üõ†Ô∏è Calling tool: {tool_name} with {tool_args}\")\n",
    "                    \n",
    "                    # Execute tool via MCP\n",
    "                    result = await session.call_tool(tool_name, tool_args)\n",
    "                    print(f\"‚úÖ Tool Result: {result.content}\")\n",
    "                    \n",
    "                    # Final response from LLM\n",
    "                    final_response = ollama.chat(\n",
    "                        model=model,\n",
    "                        messages=[\n",
    "                            {'role': 'user', 'content': user_prompt},\n",
    "                            response['message'],\n",
    "                            {'role': 'tool', 'content': str(result.content), 'name': tool_name}\n",
    "                        ]\n",
    "                    )\n",
    "                    return final_response['message']['content']\n",
    "            \n",
    "            return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609f49a",
   "metadata": {},
   "source": [
    "### Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc616abc",
   "metadata": {},
   "source": [
    "`async def run_mcp_query(user_prompt, MODEL):` \n",
    "- The word async (asynchronous) just means your computer won't freeze up while it waits for the process to finish. It can go do other chores, like a person putting a phone on speaker while waiting on hold.\n",
    "- The Layman Translation: \"I am preparing to ask my assistant a question (the user_prompt), and I'm willing to wait patiently for the answer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fb515",
   "metadata": {},
   "source": [
    "`async with stdio_client(server_params) as (read, write):`\n",
    "- This step builds the actual physical/digital pipe between your main program and the hidden tool server. The server_params are like the phone number or the address of the office.\n",
    "- The Layman Translation: \"I am plugging a secure phone line directly into the assistant's office. I have an earpiece to listen (read) and a microphone to speak (write).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a16179",
   "metadata": {},
   "source": [
    "`async with ClientSession(read, write) as session:`\n",
    "- Just having a wire isn't enough; the computers need rules on how to format their messages. A ClientSession wraps the raw wire in a polite protocol (like agreeing to say \"Over\" when you finish speaking on a walkie-talkie).\n",
    "- The Layman Translation: \"Now that the wire is connected, we are agreeing to speak the same language so we don't talk over each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d3eee",
   "metadata": {},
   "source": [
    "`await session.initialize()`\n",
    "- Before you can ask your actual question or ask the server to run tools, you have to do a digital handshake. This proves the server is awake, functioning, and ready to accept commands.\n",
    "- The Layman Translation: \"I am saying 'Hello, are you there and ready to work?' and I will wait (await) until you say 'Yes, I am ready!'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9df04",
   "metadata": {},
   "source": [
    "Let us now try to use our MCP tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 5 + 5?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01836b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await run_mcp_query(MCP_SERVER_PATH,query, SMALL_MODEL)\n",
    "print(f\"LLM Final Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfaa18f",
   "metadata": {},
   "source": [
    "You will see above that the `SMALL_MODEL` fails to give the answer to a very simple question. Though the process successfully uses the `Math` MCP tool and the result was also calculated correctly as can be seen in the 2nd line of the output under `text='10'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5cf46",
   "metadata": {},
   "source": [
    "Lets now try the same query with the `LARGE_MODEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await run_mcp_query(MCP_SERVER_PATH, query, LARGE_MODEL)\n",
    "print(f\"LLM Final Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21f4bd",
   "metadata": {},
   "source": [
    "## Run a more complex MCP servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e01dc",
   "metadata": {},
   "source": [
    "There is another server file `mcp_tools_server.py` inside the `mcp_servers` folder. \n",
    "\n",
    "This mcp server scripts consists of two tools.\n",
    "1. Web Search Tool - using DuckDuckGo \n",
    "2. Reading a local file Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99971d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_SERVER_PATH = os.path.abspath(\"./mcp_servers/mcp_tools_server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e4b45",
   "metadata": {},
   "source": [
    "Lets first try to use the `web_search` tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a web search:\n",
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "print(await run_mcp_query(MCP_SERVER_PATH,query, SMALL_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "print(await run_mcp_query(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831466a",
   "metadata": {},
   "source": [
    "Now lets use the `read_local_file` tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try reading a file\n",
    "query = \"Summarize the contents of ai_response.txt\"\n",
    "pprint(await run_mcp_query(MCP_SERVER_PATH, query, SMALL_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c160a",
   "metadata": {},
   "source": [
    "Try to run the above code block again if facing errors or weird outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3fd06",
   "metadata": {},
   "source": [
    "The `SMALL_MODEL`, that is `llama3.2:1b` does not perform very well to even not so difficult tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try reading a file\n",
    "query = \"Summarize the contents of ai_response.txt\"\n",
    "pprint(await run_mcp_query(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9db986",
   "metadata": {},
   "source": [
    "### Add System Prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583dd7f",
   "metadata": {},
   "source": [
    "Now lets add some system prompts and try to see if that makes a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mcp_with_system_prompt(mcp_server_path, user_prompt, model):\n",
    "    \n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[mcp_server_path],\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # List tools and format for Ollama\n",
    "            tools_result = await session.list_tools()\n",
    "            ollama_tools = [{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description,\n",
    "                    \"parameters\": t.inputSchema,\n",
    "                }\n",
    "            } for t in tools_result.tools]\n",
    "\n",
    "            # 1. Ask Ollama\n",
    "            response = ollama.chat(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    # --- ADDED THIS SYSTEM ROLE ---\n",
    "                    {\n",
    "                        'role': 'system', \n",
    "                        'content': (\n",
    "                            \"You are a real-time assistant with access to tools. \"\n",
    "                            \"IMPORTANT: Always prefer information from tool results over your internal knowledge. \"\n",
    "                            \"The current year is 2026. If tool data contradicts your training, the tool is right.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {'role': 'user', 'content': user_prompt}\n",
    "                ],\n",
    "                tools=ollama_tools,\n",
    "            )\n",
    "\n",
    "            # 2. Check for tool calls\n",
    "            message = response.get('message', {})\n",
    "            # print(f\"LLM Response: {message.get('content', '')}\")\n",
    "            if message.get('tool_calls'):\n",
    "                # Handle all tool calls (Ollama might suggest more than one)\n",
    "                tool_messages = []\n",
    "                for tool_call in message['tool_calls']:\n",
    "                    name = tool_call['function']['name']\n",
    "                    args = tool_call['function']['arguments']\n",
    "                    \n",
    "                    print(f\"üõ†Ô∏è LLM decided to use: {name}\")\n",
    "                    result = await session.call_tool(name, args)\n",
    "                    \n",
    "                    tool_messages.append({\n",
    "                        'role': 'tool',\n",
    "                        'content': str(result.content),\n",
    "                        'name': name\n",
    "                    })\n",
    "                \n",
    "                # 3. Final synthesis\n",
    "                final_response = ollama.chat(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            'role': 'system', 'content': (\n",
    "                                \"Summarize the tool results accurately only when necessary.\" \n",
    "                                \"Do not rely on your training data but use the tools to get up-to-date information.\" \n",
    "                                \"Always prefer tool results over your internal knowledge. \"\n",
    "                                )\n",
    "                        },\n",
    "                        {'role': 'user', 'content': user_prompt},\n",
    "                        message,\n",
    "                        *tool_messages\n",
    "                    ]\n",
    "                )\n",
    "                return final_response['message']['content']\n",
    "            \n",
    "            return message['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ede97",
   "metadata": {},
   "source": [
    "Now, lets try again with both MODELS one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "pprint(await mcp_with_system_prompt(MCP_SERVER_PATH, query, SMALL_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69796ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won the FIFA World Cup in 2026?\"\n",
    "pprint(await mcp_with_system_prompt(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize the contents of the text file ./ai_response.txt\"\n",
    "pprint(await mcp_with_system_prompt(MCP_SERVER_PATH,query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a7c11",
   "metadata": {},
   "source": [
    "If you are getting json responses try changing system prompt. \n",
    "\n",
    "<details>\n",
    "    <summary>Try adding this (check only if needed)</summary>\n",
    "    \"You are an MCP Assistant. If you need information, use a tool. DO NOT write raw JSON in your response. If you call a tool, use the proper tool-call format.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb8ccc",
   "metadata": {},
   "source": [
    "### Getting simple python code from web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f76d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Search for python code for calculating the first 10 numbers of the Fibonacci sequence.\"\n",
    "print(await mcp_with_system_prompt(MCP_SERVER_PATH, query, LARGE_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d86dd",
   "metadata": {},
   "source": [
    "Copy paste the above code in a python file and name it as `fibonacci_numbers.py`. Create a folder `scripts` and move the python script under the `scripts` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bea26a",
   "metadata": {},
   "source": [
    "Try running this python script in the terminal to check if the script is working or not\n",
    "\n",
    "```bash\n",
    "python scripts/fibonacci_numbers.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
